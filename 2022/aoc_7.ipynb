{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ce1f47-fa92-4dff-9159-7a03e08e4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Column\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0682ef-5906-47eb-ace9-79ada1fc7eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/08 00:28:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"AoC_2022_7[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5abcc04-b5dc-446c-a1d7-db48641d3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = spark.read.text(\"./data/aoc_7.txt\", wholetext=True).withColumn(\"filename\", F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2928849-e78f-4394-8c4f-c31c7a55e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Complete paths for Files\n",
    "\n",
    "def remove_last_element(arr: Column) -> Column:\n",
    "    return F.when(F.size(arr) == 1, arr).otherwise(F.slice(arr, F.lit(1), F.size(arr) - 1))\n",
    "\n",
    "def construct_accumulator(path: Column, absolute_paths: Column) -> Column:\n",
    "    return F.struct(F.lit(path).cast(\"array<string>\").alias(\"path\"), absolute_paths.cast(\"array<struct<path: array<string>, size: bigint>>\").alias(\"absolute_paths\"))\n",
    "\n",
    "def process_file_listing(acc, x: Column) -> Column:\n",
    "    file_parts = F.split(x, \" \")\n",
    "    size = file_parts[0].cast(\"bigint\").alias(\"size\")\n",
    "    file_struct = F.array(F.struct(acc.path.alias(\"path\"), size))\n",
    "    return construct_accumulator(acc.path, F.concat(acc.absolute_paths, file_struct))\n",
    "\n",
    "def process_listing(acc: Column, x: Column) -> Column:\n",
    "    is_dir = x.startswith(\"dir\")\n",
    "    return F.when(is_dir, acc).otherwise(process_file_listing(acc, x))\n",
    "\n",
    "def process_cd(acc: Column, command_parts: Column) -> Column:\n",
    "    is_absolute = command_parts[2].startswith(\"/\")\n",
    "    is_level_up = command_parts[2] == F.lit(\"..\")\n",
    "    \n",
    "    replace_level_up = remove_last_element(acc.path)\n",
    "    \n",
    "    absolute_accum = construct_accumulator(F.array(command_parts[2]), acc.absolute_paths)\n",
    "    level_up_accum = construct_accumulator(replace_level_up, acc.absolute_paths)\n",
    "    \n",
    "    relative_accum = construct_accumulator(F.concat(acc.path, F.array(command_parts[2])), acc.absolute_paths)\n",
    "    return (F.when(is_absolute, absolute_accum)\n",
    "            .when(is_level_up, level_up_accum)\n",
    "            .otherwise(relative_accum))\n",
    "    \n",
    "\n",
    "def process_command(acc: Column, x: Column) -> Column:\n",
    "    command_parts = F.split(x, \" \")\n",
    "    is_cd = command_parts[1] == F.lit(\"cd\")\n",
    "    is_ls = command_parts[1] == F.lit(\"ls\")\n",
    "    return F.when(is_ls, acc).otherwise(process_cd(acc, command_parts))\n",
    "    \n",
    "\n",
    "def merge(acc: Column, x: Column) -> Column:\n",
    "    is_command = x.startswith(\"$\")\n",
    "    return F.when(is_command, process_command(acc, x)).otherwise(process_listing(acc, x))\n",
    "\n",
    "def path(splits: Column) -> Column:\n",
    "    init_accum = construct_accumulator(F.array(F.lit(\"\")), F.array(F.lit(None)))\n",
    "    return F.aggregate(splits, init_accum, merge)\n",
    "\n",
    "def construct_complete_paths(value: Column) -> Column:\n",
    "    splits = F.split(value, \"\\n\")\n",
    "    non_null_paths = F.filter(path(splits).absolute_paths, lambda x: x.isNotNull())\n",
    "    return non_null_paths.alias(\"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d83f4db-7099-4627-ad8d-00b6d8b6e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contributions(arr: Column) -> Column:\n",
    "    def merger(acc, x):\n",
    "        next_v = F.concat(F.element_at(acc, -1), x, F.lit(\"/\"))\n",
    "        return F.when(acc.isNull(), F.array(x)).otherwise(F.concat(acc, F.array(next_v)))\n",
    "    return F.aggregate(arr, F.lit(None).cast(\"array<string>\"), merger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8086e42f-f446-479e-96f1-0298664ca109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(result=1517599)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# man_inputs = \"$ cd /\\n$ ls\\ndir a\\n14848514 b.txt\\n8504156 c.dat\\ndir d\\n$ cd a\\n$ ls\\ndir e\\n29116 f\\n2557 g\\n62596 h.lst\\n$ cd e\\n$ ls\\n584 i\\n$ cd ..\\n$ cd ..\\n$ cd d\\n$ ls\\n4060174 j\\n8033020 d.log\\n5626152 d.ext\\n7214296 k\"\n",
    "\n",
    "man_inputs = \"$ cd /\\n$ ls\\ndir a\\n14848514 b.txt\\n8504156 c.dat\\ndir d\\n$ cd a\\n$ ls\\ndir e\\n29116 f\\n2557 g\\n62596 h.lst\\n$ cd e\\n$ ls\\n584 i\\n$ cd ..\\n$ cd ..\\n$ cd d\\n$ ls\\n4060174 j\\n8033020 d.log\\n5626152 d.ext\\n7214296 k\\ndir a\\n$ cd a\\n$ ls\\n6259611 anested\"\n",
    "\n",
    "# file = F.explode(construct_complete_paths(F.lit(man_inputs))).alias(\"file\")\n",
    "\n",
    "file = F.explode(construct_complete_paths(F.col(\"value\"))).alias(\"file\")\n",
    "\n",
    "file_size = inputs.select(\"*\", file).select(\"filename\", \"file.*\")\n",
    "\n",
    "\n",
    "dirs_level = file_size.select(\"*\", F.explode(contributions(F.col(\"path\"))).alias(\"dir\"))\n",
    "total_size = dirs_level.groupBy(\"filename\", \"dir\").agg(F.sum(\"size\").alias(\"totalsize\"))\n",
    "\n",
    "total_size.where(\"totalsize <= 100000\").select(F.sum(\"totalsize\").alias(\"result\")).head()\n",
    "\n",
    "# dirs_level.show(200, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad2a2997-3d56-49a5-826f-499399be7c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(min(totalsize)=2481982)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_space = 70000000\n",
    "unused_space = total_space - total_size.where(\"dir == '/'\").head().totalsize\n",
    "spark_for_upgrade = 30000000\n",
    "space_to_free = 30000000 - unused_space\n",
    "total_size.where(f\"totalsize >= {space_to_free}\").select(F.min(\"totalsize\")).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a56ff9-8c14-46fa-87f4-3394f465e766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b4c63cb2d918b6f68ee143ad2028ff4f90f6ba246f1c2fb2516c69c9e93e5d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
